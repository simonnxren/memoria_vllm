# memoria_vllm - Environment Configuration
# Copy this to .env and customize values for your deployment

# =============================================================================
# MODEL CONFIGURATION - HuggingFace model identifiers
# =============================================================================
MODEL_EMBED_NAME=Qwen/Qwen3-Embedding-0.6B
MODEL_GEN_NAME=Qwen/Qwen3-4B-Thinking-2507-FP8

# =============================================================================
# SERVICE PORTS
# =============================================================================
ROUTER_PORT=8200
VLLM_EMBED_PORT=8100
VLLM_GEN_PORT=8101

# =============================================================================
# VLLM SERVICE URLS - Docker networking
# =============================================================================
VLLM_EMBED_URL=http://vllm-embedding:8100
VLLM_GEN_URL=http://vllm-generation:8101

# =============================================================================
# GPU MEMORY ALLOCATION - Fraction of total GPU memory (0.0-1.0)
# =============================================================================
VLLM_EMBED_GPU_MEMORY=0.3  # 30% for embedding
VLLM_GEN_GPU_MEMORY=0.6    # 60% for generation (total â‰¤0.95)

# =============================================================================
# EMBEDDING INSTANCE - Performance Tuning
# =============================================================================
VLLM_EMBED_MAX_NUM_SEQS=256  # Concurrent sequences (128-512 recommended)
VLLM_EMBED_MAX_BATCHED_TOKENS=8192  # Batch size (4096-16384 recommended)
# Pooler config: MEAN pooling, normalized, chunked processing for long texts
VLLM_EMBED_POOLER_CONFIG='{"pooling_type":"MEAN","normalize":true,"enable_chunked_processing":true,"max_embed_len":8192}'

# =============================================================================
# GENERATION INSTANCE - Performance Tuning
# =============================================================================
VLLM_GEN_MAX_NUM_SEQS=128  # Concurrent sequences (64-256 recommended)
VLLM_GEN_MAX_BATCHED_TOKENS=16384  # Batch size (8192-32768 recommended)
VLLM_GEN_MAX_MODEL_LEN=8192  # Max context length (model-dependent)
VLLM_GEN_SCHEDULER_POLICY=fcfs  # fcfs or priority

# =============================================================================
# LOGGING
# =============================================================================
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_FORMAT=json  # json or text
MAX_LOG_LEN=  # Max chars/tokens to log (blank = unlimited)

# =============================================================================
# HTTP CLIENT SETTINGS
# =============================================================================
HTTP_TIMEOUT=30  # Request timeout (seconds)
HEALTH_CHECK_TIMEOUT=5  # Health check timeout (seconds)

# =============================================================================
# ROUTER CONFIGURATION
# =============================================================================
ENABLE_METRICS=false  # Prometheus metrics at /metrics
ENABLE_REQUEST_LOGGING=false  # Detailed request/response logging
